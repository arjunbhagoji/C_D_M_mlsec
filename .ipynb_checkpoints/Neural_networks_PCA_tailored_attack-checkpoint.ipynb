{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions...\n",
      "Final results:\n",
      "  test loss:\t\t\t0.146091\n",
      "  test accuracy:\t\t95.47 %\n",
      "0.20148 0.17776 0.20742\n",
      "0.153841775803 0.136297375086 0.158995039457\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "#from lasagne.regularization import l2\n",
    "\n",
    "def load_dataset():\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def build_custom_mlp(input_var=None, DEPTH=2, WIDTH=800, DROP_INPUT=.2,\n",
    "                     DROP_HIDDEN=.5):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    if DROP_INPUT:\n",
    "        network = lasagne.layers.dropout(network, p=DROP_INPUT)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.sigmoid\n",
    "    for _ in range(DEPTH):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "                network, WIDTH, nonlinearity=nonlin)\n",
    "        if DROP_HIDDEN:\n",
    "            network = lasagne.layers.dropout(network, p=DROP_HIDDEN)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 10, nonlinearity=softmax)\n",
    "    return network\n",
    "\n",
    "def build_hidden_fc(input_var=None, WIDTH=100):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    in_layer = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    #if DROP_INput:\n",
    "    #    network = lasagne.layers.dropout(network, p=DROP_INput)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.sigmoid\n",
    "    #for _ in range(DEPTH):\n",
    "    layer_1 = lasagne.layers.DenseLayer(in_layer, WIDTH, nonlinearity=nonlin)\n",
    "    layer_2 = lasagne.layers.DenseLayer(layer_1, WIDTH, nonlinearity=nonlin)\n",
    "    #    if DROP_HIDden:\n",
    "    #        network = lasagne.layers.dropout(network, p=DROP_HIDden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(layer_2, 10, nonlinearity=softmax)\n",
    "    return network\n",
    "    #, layer_1, layer_2\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # 2 Convolutional layers with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 64 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=64, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=64, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(network, num_units=200,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.DenseLayer(network, num_units=200,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(network, num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "\n",
    "NUM_EPOCHS=500\n",
    "DEPTH=2\n",
    "WIDTH=100\n",
    "DROP_IN=0.2\n",
    "DROP_HID=0.5\n",
    "network = build_custom_mlp(input_var, int(DEPTH), int(WIDTH),\n",
    "                            float(DROP_IN), float(DROP_HID))\n",
    "\n",
    "\n",
    "#if hidden_layers==0:\n",
    "# DEPTH=2\n",
    "# WIDTH=800\n",
    "# DROP_IN=0.2\n",
    "# DROP_HID=0.5\n",
    "# network = build_custom_mlp(input_var, int(DEPTH), int(WIDTH),\n",
    "#                             float(DROP_IN), float(DROP_HID))\n",
    "    #lambda_network=1e-4\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    # prediction = lasagne.layers.get_output(network)\n",
    "    # loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    #l2_penalty=lasagne.regularization.regularize_layer_params(\n",
    "                                                    #network,l2)*lambda_network\n",
    "    #loss=loss+l2_penalty/10\n",
    "    # loss = loss.mean()\n",
    "    #myfile=open('FSG_results.txt','a')\n",
    "    #myfile.write('Model: FC10('+str(lambda_network)+')'+'\\n')\n",
    "# elif hidden_layers!=0:\n",
    "#     WIDTH=100\n",
    "#     network=build_hidden_fc(input_var,int(WIDTH))\n",
    "    #, layer_1, layer_2\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    # prediction = lasagne.layers.get_output(network)\n",
    "    # loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    #layers={layer_1:1e-7,layer_2:1e-7,network:1e-7}\n",
    "    #l2_penalty=lasagne.regularization.regularize_layer_params_weighted(layers,\n",
    "                                                                            #l2)\n",
    "    #loss=loss\n",
    "    #+l2_penalty\n",
    "    # loss = loss.mean()\n",
    "    # myfile=open('FSG_results.txt','a')\n",
    "    # myfile.write('Model: FC10_'+str(hidden_layers)+'_'+str(WIDTH)+'\\n')\n",
    "\n",
    "#network=build_cnn(input_var)\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss=loss.mean()\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "eval_fn=theano.function([input_var,target_var],loss)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "predict_fn=theano.function([input_var],T.argmax(test_prediction, axis=1))\n",
    "\n",
    "model_exist_flag=1\n",
    "\n",
    "script_dir=os.getcwd()\n",
    "\n",
    "if model_exist_flag==1:\n",
    "#    And load them again later on like this:\n",
    "    rel_path_m=\"Models/\"\n",
    "    abs_path_m=os.path.join(script_dir,rel_path_m)\n",
    "    with np.load(abs_path_m+'model_FC10_'+str(DEPTH)+'_'+str(WIDTH)+'_drop.npz') as f:\n",
    "            param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    # with np.load(abs_path_m+'model_cnn_9_layers_papernot.npz') as f:\n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(network, param_values)\n",
    "# elif model_exist_flag==1 and hidden_layers!=0:\n",
    "#     with np.load('model_FC10_'+str(hidden_layers)+'_'+str(WIDTH)+'.npz') as f:\n",
    "#         param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "#     lasagne.layers.set_all_param_values(network, param_values)\n",
    "elif model_exist_flag==0:\n",
    "        # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 1, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "        #print(\"{}\".format(inputs))\n",
    "        #print(\"{}\".format(targets+1))\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, NUM_EPOCHS, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "    rel_path_m=\"Models/\"\n",
    "    abs_path_m=os.path.join(script_dir,rel_path_m)\n",
    "\n",
    "    np.savez('model_FC10_'+str(DEPTH)+'_'+str(WIDTH)+'_drop.npz',\n",
    "            *lasagne.layers.get_all_param_values(network))\n",
    "\n",
    "    # np.savez(abs_path_m+'model_cnn_9_layers_papernot.npz',\n",
    "    #         *lasagne.layers.get_all_param_values(network))\n",
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))\n",
    "\n",
    "avg_test_acc=test_acc/test_batches*100\n",
    "\n",
    "# Writing the test results out to a file\n",
    "rel_path_o=\"Output_data/\"\n",
    "abs_path_o=os.path.join(script_dir,rel_path_o)\n",
    "myfile=open(abs_path_o+'MNIST_test_perform.txt','a')\n",
    "myfile.write('Model: FC10_'+str(DEPTH)+'_'+str(WIDTH)+\n",
    "                '_drop'+'\\n')\n",
    "#myfile.write('Model: model_cnn_9_layers_papernot'+'\\n')\n",
    "myfile.write(\"reduced_dim: \"+\"N.A.\"+\"\\n\"+\"Epochs: \"\n",
    "            +str(NUM_EPOCHS)+\"\\n\"+\"Test accuracy: \"\n",
    "            +str(avg_test_acc)+\"\\n\")\n",
    "myfile.write(\"#####################################################\"+\n",
    "                \"####\"+\"\\n\")\n",
    "myfile.close()\n",
    "\n",
    "# if hidden_layers==0:\n",
    "#     np.savez('model_FC10('+str(lambda_network)+')'+'.npz',\n",
    "#                 *lasagne.layers.get_all_param_values(network))\n",
    "# elif hidden_layers!=0:\n",
    "#     np.savez('model_FC10_'+str(hidden_layers)+'_'+str(WIDTH)+'.npz',\n",
    "#             *lasagne.layers.get_all_param_values(network))\n",
    "\n",
    "#Computing adversarial examples using fast sign gradient method\n",
    "# req_gradient=T.grad(loss,input_var)\n",
    "# grad_function=theano.function([input_var,target_var],req_gradient)\n",
    "confidence=theano.function([input_var],test_prediction)\n",
    "#from matplotlib import pyplot as plt\n",
    "#%matplotlib inline\n",
    "rel_path_o=\"Output_data/\"\n",
    "abs_path_o=os.path.join(script_dir,rel_path_o)\n",
    "# Variables and loop for finding adversarial examples from traning set\n",
    "# plotfile=open(abs_path_o+'PCA_attack_MNIST_data_hidden_'+str(DEPTH)+'_'\n",
    "#                  +str(WIDTH)+'_'+'.txt','a')\n",
    "X_train=X_train.reshape((50000,784))\n",
    "\n",
    "rd=100\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "PCA_in_train=X_train.reshape(50000,784)\n",
    "PCA_in_val=X_val.reshape(10000,784)\n",
    "PCA_in_test=X_test.reshape(10000,784)\n",
    "\n",
    "pca=PCA(n_components=rd)\n",
    "pca_train=pca.fit(PCA_in_train)\n",
    "\n",
    "#Transforming the training, validation and test data\n",
    "X_train_dr=pca.transform(PCA_in_train).reshape((50000,rd))\n",
    "\n",
    "PCA_vectors=pca.components_\n",
    "PCA_vectors=np.transpose(np.array(PCA_vectors))\n",
    "PCA_abs=np.absolute(PCA_vectors)\n",
    "\n",
    "scores_abs=np.zeros(784)\n",
    "for i in range(784):\n",
    "    for j in range(100):\n",
    "        scores_abs[i]=scores_abs[i]+np.sum(X_train[:,i]*PCA_abs[i,j])\n",
    "\n",
    "top_features=np.argsort(scores_abs)[::-1][:40]\n",
    "\n",
    "zero_array=np.zeros((50000,784))\n",
    "\n",
    "for i in range(50000):\n",
    "    np.put(zero_array[i,:],top_features,1)\n",
    "\n",
    "adv_x=X_train+1.0*zero_array\n",
    "\n",
    "count_tot=0.0\n",
    "count_wrong=0.0\n",
    "conf_wrong=0.0\n",
    "count_abs_wrong=0.0\n",
    "conf_abs=0.0\n",
    "count_adv=0.0\n",
    "conf_adv=0.0\n",
    "for i in range(50000):\n",
    "    input_curr=(X_train[i]).reshape((1,1,28,28))\n",
    "    y_curr=y_train[i].reshape((1,))\n",
    "    ini_class=predict_fn(input_curr)\n",
    "    #x_adv=(x_ini-1.0*(clf.coef_[ini_class[0],:]/(np.linalg.norm(clf.coef_[ini_class[0],:])))).reshape((1,784))\n",
    "    x_adv=adv_x[i,:].reshape((1,1,28,28))\n",
    "    prediction_curr=predict_fn(x_adv)\n",
    "    if prediction_curr!=predict_fn(input_curr):\n",
    "        count_wrong=count_wrong+1\n",
    "        conf_wrong=conf_wrong+confidence(x_adv)[0][prediction_curr[0]]\n",
    "    if prediction_curr!=y_curr and y_curr==predict_fn(input_curr):\n",
    "        count_adv=count_adv+1\n",
    "        conf_adv=conf_adv+confidence(x_adv)[0][prediction_curr[0]]\n",
    "    if prediction_curr!=y_curr:\n",
    "        count_abs_wrong=count_abs_wrong+1\n",
    "        conf_abs=conf_abs+confidence(x_adv)[0][prediction_curr[0]]\n",
    "    count_tot=count_tot+1\n",
    "print count_wrong/50000.0, count_adv/50000.0, count_abs_wrong/50000.0\n",
    "print conf_wrong/50000.0, conf_adv/50000.0, conf_abs/50000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import sys,os\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.regularization import l2\n",
    "from sklearn.decomposition import PCA\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "# Defining required functions\n",
    "def load_dataset():\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        #if not os.path.exists(filename):\n",
    "        #    download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        #if not os.path.exists(filename):\n",
    "        #    download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    script_dir=os.getcwd()\n",
    "    rel_path=\"Input_data/\"\n",
    "    abs_path=os.path.join(script_dir,rel_path)\n",
    "    X_train = load_mnist_images(abs_path+'train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels(abs_path+'train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images(abs_path+'t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels(abs_path+'t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# ############################# Batch iterator ###############################\n",
    "# This is just a simple helper function iterating over training data in\n",
    "# mini-batches of a particular size, optionally in random order.\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "def build_custom_mlp(input_var, depth, WIDTH, drop_input,\n",
    "                     drop_hidden,rd):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, rd),\n",
    "                                        input_var=input_var)\n",
    "    if drop_input:\n",
    "        network = lasagne.layers.dropout(network, p=drop_input)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.sigmoid\n",
    "    for _ in range(depth):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "                network, WIDTH, nonlinearity=nonlin)\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(network, p=drop_hidden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 10, nonlinearity=softmax)\n",
    "    return network\n",
    "\n",
    "def build_cnn(input_var,rd):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, rd),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # 2 Convolutional layers with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv1DLayer(\n",
    "            network, num_filters=32, filter_size=5,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.Conv1DLayer(\n",
    "            network, num_filters=32, filter_size=5,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool1DLayer(network, pool_size=2)\n",
    "\n",
    "    # Another convolution with 64 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv1DLayer(\n",
    "            network, num_filters=64, filter_size=5,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.Conv1DLayer(\n",
    "            network, num_filters=64, filter_size=5,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool1DLayer(network, pool_size=2)\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(network, num_units=200,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.DenseLayer(network, num_units=200,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(network, num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "def pca_main(rd):\n",
    "    rd=rd\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test=load_dataset()\n",
    "    #X_adv=np.genfromtxt(abs_path_o+'adv_examples_cnn_9_layers_papernot.txt',delimiter=',')\n",
    "    #Reshaping for PCA function\n",
    "    PCA_in_train=X_train.reshape(50000,784)\n",
    "    PCA_in_val=X_val.reshape(10000,784)\n",
    "    PCA_in_test=X_test.reshape(10000,784)\n",
    "    # PCA_in_adv_train=X_adv[:500000,:].reshape(50000,784,10)\n",
    "    # PCA_in_adv_test=X_adv[500000:,:].reshape(10000,784,10)\n",
    "\n",
    "    print(\"Doing PCA over the training data\")\n",
    "    #Fitting the PCA model on training data\n",
    "    pca=PCA(n_components=rd)\n",
    "    pca_train=pca.fit(PCA_in_train)\n",
    "    # X_adv_dr_train=np.zeros((50000,784,10))\n",
    "    # X_adv_dr_test=np.zeros((10000,784,10))\n",
    "    print (\"Transforming the training, validation and test data\")\n",
    "    X_train_dr=pca.transform(PCA_in_train).reshape((50000,1,rd))\n",
    "    X_test_dr=pca.transform(PCA_in_test).reshape((10000,1,rd))\n",
    "    X_val_dr=pca.transform(PCA_in_val).reshape((10000,1,rd))\n",
    "    # for i in range(1):\n",
    "    #     X_adv_dr_train[:,:,i]=pca.transform(PCA_in_adv_train[:,:,i])\n",
    "    #     X_adv_dr_test[:,:,i]=pca.transform(PCA_in_adv_test[:,:,i])\n",
    "\n",
    "\n",
    "    ### Neural network learning with DR examples\n",
    "    # Prepare Theano variables for inputs and targets\n",
    "    input_var = T.tensor3('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "    # Create neural network model (depending on if hidden layers exist or not)\n",
    "    print(\"Building model and compiling functions...\")\n",
    "    #depth, width, drop_in, drop_hid = model.split(':', 1)[1].split(',')\n",
    "    network=build_custom_mlp(input_var, int(DEPTH), int(WIDTH),\n",
    "                                float(DROP_IN), float(DROP_HID),rd)\n",
    "    #network = build_cnn(input_var,rd)\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.1, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    eval_fn=theano.function([input_var,target_var],loss)\n",
    "\n",
    "    # Compile a function computing the validation loss and accuracy:\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "    # Compile a function giving the network's prediction on any input\n",
    "    predict_fn=theano.function([input_var],T.argmax(test_prediction, axis=1))\n",
    "    #Probability vector for each output\n",
    "    confidence=theano.function([input_var],test_prediction)\n",
    "\n",
    "    # Loading the trained model\n",
    "    rel_path_m=\"Models/\"\n",
    "    abs_path_m=os.path.join(script_dir,rel_path_m)\n",
    "    abs_path_m=os.path.join(abs_path_m,'model_FC10_'+str(DEPTH)\n",
    "                                 +'_'+str(WIDTH)+'_PCA_'+str(rd)+'_drop'+'.npz')\n",
    "#     with np.load(abs_path_m+'model_cnn_9_layers_papernot'\n",
    "#                                 +'_PCA_'+str(rd)+'.npz') as f:\n",
    "#         param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(network, param_values)\n",
    "\n",
    "    # After training, we compute and print the test error:\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterate_minibatches(X_test_dr, y_test, 500, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_acc / test_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEPTH=2\n",
    "WIDTH=100\n",
    "DROP_IN=0.2\n",
    "DROP_HID=0.5\n",
    "script_dir=os.getcwd()\n",
    "rel_path_o=\"Output_data/\"\n",
    "abs_path_o=os.path.join(script_dir,rel_path_o)\n",
    "\n",
    "#rd_list=[100,50,30]\n",
    "rd=100\n",
    "#num_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-196a1ac4195a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpca_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-2cbca98c846c>\u001b[0m in \u001b[0;36mpca_main\u001b[1;34m(rd)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[0mrd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading data...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m     \u001b[1;31m#X_adv=np.genfromtxt(abs_path_o+'adv_examples_cnn_9_layers_papernot.txt',delimiter=',')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;31m#Reshaping for PCA function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-2cbca98c846c>\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# We can now download and read the training and test set images and labels.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mscript_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mrel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Input_data/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mabs_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscript_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "pca_main(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

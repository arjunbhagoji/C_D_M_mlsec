{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 960 (CNMeM is disabled, cuDNN 5005)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Building model and compiling functions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/lasagne/layers/conv.py:489: UserWarning: The `image_shape` keyword argument to `tensor.nnet.conv2d` is deprecated, it has been renamed to `input_shape`.\n",
      "  border_mode=border_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test loss:\t\t\t0.052260\n",
      "  test accuracy:\t\t98.98 %\n",
      "  test confidence:\t\t99.62 %\n",
      "[[  1.10656787e-15   9.99315739e-01   8.44254666e-11 ...,   2.61233896e-10\n",
      "    6.83647173e-04   1.77513414e-18]\n",
      " [  9.95535394e-22   3.50648834e-15   8.71230164e-17 ...,   1.51274865e-15\n",
      "    1.27253390e-11   6.59026655e-09]\n",
      " [  1.26022065e-19   2.80297161e-14   4.26890676e-17 ...,   2.38645020e-10\n",
      "    1.83193920e-19   1.39106405e-18]\n",
      " ..., \n",
      " [  1.33853874e-20   1.64113569e-12   9.83854383e-14 ...,   7.10711845e-07\n",
      "    6.82728451e-13   7.21098363e-01]\n",
      " [  7.29023432e-19   9.99999881e-01   1.15943225e-14 ...,   3.98471194e-08\n",
      "    2.70369258e-13   6.71004141e-10]\n",
      " [  1.33853874e-20   1.64113569e-12   9.83854383e-14 ...,   7.10711845e-07\n",
      "    6.82728451e-13   7.21098363e-01]]\n",
      "0.772599995136\n",
      "1\n",
      "[[  9.29784201e-12   3.55371700e-12   4.16419820e-15 ...,   5.73326719e-14\n",
      "    4.42601786e-14   3.03610054e-10]\n",
      " [  9.29784201e-12   3.55371700e-12   4.16419820e-15 ...,   5.73326719e-14\n",
      "    4.42601786e-14   3.03610054e-10]\n",
      " [  1.28999567e-13   2.61442198e-16   4.51847017e-01 ...,   7.19947565e-18\n",
      "    5.48152983e-01   2.32993866e-18]\n",
      " ..., \n",
      " [  3.77443563e-11   8.90122592e-01   5.13983377e-06 ...,   5.39543887e-12\n",
      "    1.09872237e-01   1.16569208e-14]\n",
      " [  8.44308957e-14   9.99879241e-01   8.51299933e-13 ...,   5.25860022e-09\n",
      "    5.27228003e-05   5.33560474e-10]\n",
      " [  9.99186218e-01   5.66230853e-15   7.52850891e-17 ...,   1.88217958e-13\n",
      "    4.10894690e-18   8.13762774e-04]]\n",
      "0.754000008106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4d5b3f3b5348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;31m#     conf_adv=conf_adv+confidence(adv_x)[0][prediction_curr[0]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mprediction_curr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madv_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprediction_curr\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0my_curr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 \u001b[0mcount_abs_wrong\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount_abs_wrong\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#from lasagne.regularization import l2\n",
    "\n",
    "def load_dataset():\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    script_dir=os.getcwd()\n",
    "    rel_path=\"Input_data/\"\n",
    "    abs_path=os.path.join(script_dir,rel_path)\n",
    "    X_train = load_mnist_images(abs_path+'train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels(abs_path+'train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images(abs_path+'t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels(abs_path+'t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "def build_custom_mlp(input_var=None, DEPTH=2, WIDTH=800, DROP_INput=.2,\n",
    "                     DROP_HIDden=.5):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    if DROP_INput:\n",
    "        network = lasagne.layers.dropout(network, p=DROP_INput)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.sigmoid\n",
    "    for _ in range(DEPTH):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "                network, WIDTH, nonlinearity=nonlin)\n",
    "        if DROP_HIDden:\n",
    "            network = lasagne.layers.dropout(network, p=DROP_HIDden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(network, 10, nonlinearity=softmax)\n",
    "    return network\n",
    "\n",
    "def build_hidden_fc(input_var=None, WIDTH=100):\n",
    "    # By default, this creates the same network as `build_mlp`, but it can be\n",
    "    # customized with respect to the number and size of hidden layers. This\n",
    "    # mostly showcases how creating a network in Python code can be a lot more\n",
    "    # flexible than a configuration file. Note that to make the code easier,\n",
    "    # all the layers are just called `network` -- there is no need to give them\n",
    "    # different names if all we return is the last one we created anyway; we\n",
    "    # just used different names above for clarity.\n",
    "\n",
    "    # Input layer and dropout (with shortcut `dropout` for `DropoutLayer`):\n",
    "    in_layer = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    #if DROP_INput:\n",
    "    #    network = lasagne.layers.dropout(network, p=DROP_INput)\n",
    "    # Hidden layers and dropout:\n",
    "    nonlin = lasagne.nonlinearities.sigmoid\n",
    "    #for _ in range(DEPTH):\n",
    "    layer_1 = lasagne.layers.DenseLayer(in_layer, WIDTH, nonlinearity=nonlin)\n",
    "    layer_2 = lasagne.layers.DenseLayer(layer_1, WIDTH, nonlinearity=nonlin)\n",
    "    #    if DROP_HIDden:\n",
    "    #        network = lasagne.layers.dropout(network, p=DROP_HIDden)\n",
    "    # Output layer:\n",
    "    softmax = lasagne.nonlinearities.softmax\n",
    "    network = lasagne.layers.DenseLayer(layer_2, 10, nonlinearity=softmax)\n",
    "    return network\n",
    "    #, layer_1, layer_2\n",
    "\n",
    "def build_cnn(input_var=None):\n",
    "    # As a third model, we'll create a CNN of two convolution + pooling stages\n",
    "    # and a fully-connected hidden layer in front of the output layer.\n",
    "\n",
    "    # Input layer, as usual:\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                        input_var=input_var)\n",
    "    # This time we do not apply input dropout, as it tends to work less well\n",
    "    # for convolutional layers.\n",
    "\n",
    "    # 2 Convolutional layers with 32 kernels of size 5x5. Strided and padded\n",
    "    # convolutions are supported as well; see the docstring.\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=32, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    # Expert note: Lasagne provides alternative convolutional layers that\n",
    "    # override Theano's choice of which implementation to use; for details\n",
    "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
    "\n",
    "    # Max-pooling layer of factor 2 in both dimensions:\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # Another convolution with 64 5x5 kernels, and another 2x2 pooling:\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=64, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "            network, num_filters=64, filter_size=(5, 5),\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "\n",
    "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(network, num_units=200,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network = lasagne.layers.DenseLayer(network, num_units=200,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
    "    network = lasagne.layers.DenseLayer(network, num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return network\n",
    "\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "# Create neural network model (depending on first command line parameter)\n",
    "print(\"Building model and compiling functions...\")\n",
    "\n",
    "NUM_EPOCHS=50\n",
    "\n",
    "#if hidden_layers==0:\n",
    "DEPTH=2\n",
    "WIDTH=100\n",
    "# DROP_IN=0.2\n",
    "# DROP_HID=0.5\n",
    "# network = build_custom_mlp(input_var, int(DEPTH), int(WIDTH),\n",
    "#                             float(DROP_IN), float(DROP_HID))\n",
    "#network=build_hidden_fc(input_var, WIDTH=WIDTH)\n",
    "    #lambda_network=1e-4\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    # prediction = lasagne.layers.get_output(network)\n",
    "    # loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    #l2_penalty=lasagne.regularization.regularize_layer_params(\n",
    "                                                    #network,l2)*lambda_network\n",
    "    #loss=loss+l2_penalty/10\n",
    "    # loss = loss.mean()\n",
    "    #myfile=open('FSG_results.txt','a')\n",
    "    #myfile.write('Model: FC10('+str(lambda_network)+')'+'\\n')\n",
    "# elif hidden_layers!=0:\n",
    "#     WIDTH=100\n",
    "#     network=build_hidden_fc(input_var,int(WIDTH))\n",
    "    #, layer_1, layer_2\n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    # prediction = lasagne.layers.get_output(network)\n",
    "    # loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    #layers={layer_1:1e-7,layer_2:1e-7,network:1e-7}\n",
    "    #l2_penalty=lasagne.regularization.regularize_layer_params_weighted(layers,\n",
    "                                                                            #l2)\n",
    "    #loss=loss\n",
    "    #+l2_penalty\n",
    "    # loss = loss.mean()\n",
    "    # myfile=open('FSG_results.txt','a')\n",
    "    # myfile.write('Model: FC10_'+str(hidden_layers)+'_'+str(WIDTH)+'\\n')\n",
    "\n",
    "network=build_cnn(input_var)\n",
    "\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "loss=loss.mean()\n",
    "# Create update expressions for training, i.e., how to modify the\n",
    "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()\n",
    "# As a bonus, also create an expression for the classification accuracy:\n",
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "eval_fn=theano.function([input_var,target_var],loss)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "predict_fn=theano.function([input_var],T.argmax(test_prediction, axis=1))\n",
    "confidence=theano.function([input_var],test_prediction)\n",
    "conf_calc=T.mean(T.max(test_prediction, axis=1),\n",
    "                  dtype=theano.config.floatX)\n",
    "confidence_2=theano.function([input_var],conf_calc)\n",
    "correct_confidence=theano.function([input_var],T.max(test_prediction,axis=1))\n",
    "\n",
    "model_exist_flag=1\n",
    "adv_example_flag=0\n",
    "\n",
    "script_dir=os.getcwd()\n",
    "\n",
    "if model_exist_flag==1:\n",
    "    rel_path_m=\"Models/\"\n",
    "    abs_path_m=os.path.join(script_dir,rel_path_m)\n",
    "    # And load them again later on like this:\n",
    "    # with np.load(abs_path_m+'model_FC10_'+str(DEPTH)+'_'+str(WIDTH)+'_.npz') as f:\n",
    "    #         param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    with np.load(abs_path_m+'model_cnn_9_layers_papernot.npz') as f:\n",
    "        param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(network, param_values)\n",
    "# elif model_exist_flag==1 and hidden_layers!=0:\n",
    "#     with np.load('model_FC10_'+str(hidden_layers)+'_'+str(WIDTH)+'.npz') as f:\n",
    "#         param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "#     lasagne.layers.set_all_param_values(network, param_values)\n",
    "elif model_exist_flag==0:\n",
    "        # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn(inputs, targets)\n",
    "            train_batches += 1\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 1, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "        #print(\"{}\".format(inputs))\n",
    "        #print(\"{}\".format(targets+1))\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, NUM_EPOCHS, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))\n",
    "\n",
    "    rel_path_m=\"Models/\"\n",
    "    abs_path_m=os.path.join(script_dir,rel_path_m)\n",
    "    # np.savez(abs_path_m+'model_cnn_9_layers_papernot.npz',\n",
    "    #         *lasagne.layers.get_all_param_values(network))\n",
    "    np.savez(abs_path_m+'model_FC10_'+str(DEPTH)+'_'+str(WIDTH)+'_.npz',\n",
    "            *lasagne.layers.get_all_param_values(network))\n",
    "# After training, we compute and print the test error:\n",
    "test_err = 0\n",
    "test_acc = 0\n",
    "test_batches = 0\n",
    "test_conf= 0\n",
    "for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
    "    inputs, targets = batch\n",
    "    err, acc = val_fn(inputs, targets)\n",
    "    test_err += err\n",
    "    test_acc += acc\n",
    "    test_batches += 1\n",
    "for i in range(10000):\n",
    "    x_curr=X_test[i].reshape((1,1,28,28))\n",
    "    test_conf+=confidence(x_curr)[0][predict_fn(x_curr)[0]]\n",
    "\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(test_acc / test_batches * 100))\n",
    "print(\"  test confidence:\\t\\t{:.2f} %\".format(test_conf / 10000 * 100))\n",
    "\n",
    "avg_test_acc=test_acc/test_batches*100\n",
    "\n",
    "# Writing the test results out to a file\n",
    "rel_path_o=\"Output_data/\"\n",
    "abs_path_o=os.path.join(script_dir,rel_path_o)\n",
    "myfile=open(abs_path_o+'MNIST_test_perform.txt','a')\n",
    "# myfile.write('Model: FC10_'+str(DEPTH)+'_'+str(WIDTH)+\n",
    "#                 '_'+'\\n')\n",
    "myfile.write('Model: model_cnn_9_layers_papernot'+'\\n')\n",
    "myfile.write(\"reduced_dim: \"+\"N.A.\"+\"\\n\"+\"Epochs: \"\n",
    "            +str(NUM_EPOCHS)+\"\\n\"+\"Test accuracy: \"\n",
    "            +str(avg_test_acc)+\"\\n\")\n",
    "myfile.write(\"#####################################################\"+\n",
    "                \"####\"+\"\\n\")\n",
    "myfile.close()\n",
    "\n",
    "#Computing adversarial examples using fast sign gradient method\n",
    "req_gradient=T.grad(loss,input_var)\n",
    "grad_function=theano.function([input_var,target_var],req_gradient)\n",
    "#Computing and storing adv. examples\n",
    "no_of_mags=10\n",
    "train_size=50000\n",
    "test_size=10000\n",
    "adv_examples_train=np.zeros((no_of_mags,train_size,1,28,28))\n",
    "adv_examples_test=np.zeros((no_of_mags,test_size,1,28,28))\n",
    "\n",
    "#def no_pca_attack():\n",
    "rel_path_o=\"Output_data/\"\n",
    "abs_path_o=os.path.join(script_dir,rel_path_o)\n",
    "plotfile=open(abs_path_o+'FSG_MNIST_data_cnn_papernot.txt','a')\n",
    "plotfile.write('rd,Dev.,Wrong,Conf.,Adv.,Conf.,Either,Conf.,Train \\n')\n",
    "plotfile.close()\n",
    "start_time=time.time()\n",
    "mag_count=0\n",
    "for DEV_MAG in np.linspace(0.1,1.0,10):\n",
    "    count_tot=0.0\n",
    "    count_wrong=0.0\n",
    "    conf_wrong=0.0\n",
    "    count_abs_wrong=0.0\n",
    "    conf_abs=0.0\n",
    "    count_adv=0.0\n",
    "    conf_adv=0.0\n",
    "    b_count=0\n",
    "    for batch in iterate_minibatches(X_train,y_train,5000,shuffle=False):\n",
    "        input_curr, y_curr=batch\n",
    "        # Gradient w.r.t to input and current class\n",
    "        delta_x=grad_function(input_curr,y_curr)\n",
    "        # Sign of gradient\n",
    "        delta_x_sign=np.sign(delta_x)\n",
    "        #delta_x_sign=delta_x_sign/np.linalg.norm((delta_x_sign))\n",
    "        #Perturbed image\n",
    "        adv_x=input_curr+DEV_MAG*delta_x_sign\n",
    "        adv_examples_train[mag_count,b_count:b_count+5000]=adv_x\n",
    "        # Predicted class for perturbed image\n",
    "        #prediction_curr=predict_fn(adv_x)\n",
    "        # if prediction_curr!=predict_fn(input_curr):\n",
    "        cw,ca = val_fn(adv_x,y_curr)\n",
    "        count_wrong=count_wrong+ca\n",
    "        print confidence(adv_x)[predict_fn(adv_x)]\n",
    "        conf_wrong=conf_wrong+confidence_2(adv_x)\n",
    "        print ca\n",
    "        # if prediction_curr!=y_curr and y_curr==predict_fn(input_curr):\n",
    "        #     count_adv=count_adv+1\n",
    "        #     conf_adv=conf_adv+confidence(adv_x)[0][prediction_curr[0]]\n",
    "        for i in range(5000):\n",
    "            prediction_curr=predict_fn(adv_x[i].reshape((1,1,28,28)))\n",
    "            if prediction_curr!=y_curr[i]:\n",
    "                count_abs_wrong=count_abs_wrong+1\n",
    "                conf_abs=conf_abs+confidence(adv_x[i].reshape((1,1,28,28)))[0][prediction_curr[0]]\n",
    "            count_tot=count_tot+1\n",
    "        b_count=b_count+1\n",
    "        print b_count\n",
    "    # plotfile=open(abs_path_o+'FSG_MNIST_data_hidden_'+str(DEPTH)+'_'\n",
    "    #                 +str(WIDTH)+'_'+'.txt','a')\n",
    "    plotfile=open(abs_path_o+'FSG_MNIST_data_cnn_papernot.txt','a')\n",
    "    print(\"Deviation {} took {:.3f}s\".format(\n",
    "        DEV_MAG, time.time() - start_time))\n",
    "    plotfile.write('no_dr'+\",\"+str(DEV_MAG)+\",\"+\n",
    "                    str(100-count_wrong/b_count*100)+\",\"+\n",
    "                    str.format(\"{0:.3f}\",conf_wrong)+\",\"+\n",
    "                    # str(count_adv/count_tot*100)+\",\"+\n",
    "                    # str.format(\"{0:.3f}\",conf_adv/count_tot)+\",\"+\n",
    "                    str(count_abs_wrong/count_tot*100)+\",\"+\n",
    "                    str.format(\"{0:.3f}\",conf_abs/count_tot)+\",\"+str(1)+\n",
    "                    \"\\n\")\n",
    "    plotfile.close()\n",
    "    mag_count=mag_count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence(adv_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=predict_fn(adv_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=confidence(adv_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.29784201e-12,   3.55371700e-12,   4.16419820e-15, ...,\n",
       "          5.73326719e-14,   4.42601786e-14,   3.03610054e-10],\n",
       "       [  9.29784201e-12,   3.55371700e-12,   4.16419820e-15, ...,\n",
       "          5.73326719e-14,   4.42601786e-14,   3.03610054e-10],\n",
       "       [  1.28999567e-13,   2.61442198e-16,   4.51847017e-01, ...,\n",
       "          7.19947565e-18,   5.48152983e-01,   2.32993866e-18],\n",
       "       ..., \n",
       "       [  3.77443563e-11,   8.90122592e-01,   5.13983377e-06, ...,\n",
       "          5.39543887e-12,   1.09872237e-01,   1.16569208e-14],\n",
       "       [  8.44308957e-14,   9.99879241e-01,   8.51299933e-13, ...,\n",
       "          5.25860022e-09,   5.27228003e-05,   5.33560474e-10],\n",
       "       [  9.99186218e-01,   5.66230853e-15,   7.52850891e-17, ...,\n",
       "          1.88217958e-13,   4.10894690e-18,   8.13762774e-04]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999872\n",
      "1.0\n",
      "0.999991\n",
      "0.999779\n",
      "0.996855\n",
      "0.999981\n",
      "0.99993\n",
      "1.0\n",
      "0.999999\n",
      "0.663378\n",
      "0.999771\n",
      "0.999839\n",
      "0.9998\n",
      "0.829032\n",
      "0.991661\n",
      "0.999595\n",
      "0.999987\n",
      "1.0\n",
      "0.996091\n",
      "0.449432\n",
      "0.999077\n",
      "0.998126\n",
      "1.0\n",
      "0.982479\n",
      "0.99997\n",
      "0.999996\n",
      "0.999999\n",
      "0.999999\n",
      "0.999987\n",
      "0.833499\n",
      "0.999997\n",
      "0.999586\n",
      "0.998408\n",
      "0.999844\n",
      "0.999723\n",
      "1.0\n",
      "0.999991\n",
      "0.998808\n",
      "0.999348\n",
      "0.999997\n",
      "0.953586\n",
      "0.960724\n",
      "0.96407\n",
      "0.999967\n",
      "0.971688\n",
      "0.955656\n",
      "0.527251\n",
      "0.999949\n",
      "0.999998\n",
      "0.888767\n",
      "0.999842\n",
      "0.999999\n",
      "0.98867\n",
      "0.936586\n",
      "0.518965\n",
      "0.998113\n",
      "0.866464\n",
      "0.999988\n",
      "0.999989\n",
      "0.9999\n",
      "0.985562\n",
      "0.924462\n",
      "0.999999\n",
      "0.984356\n",
      "0.799834\n",
      "0.762215\n",
      "0.999924\n",
      "0.999058\n",
      "0.996096\n",
      "0.770957\n",
      "0.999872\n",
      "0.62254\n",
      "0.999996\n",
      "0.999995\n",
      "0.999999\n",
      "0.781181\n",
      "0.999889\n",
      "0.988636\n",
      "0.999845\n",
      "0.995489\n",
      "0.999989\n",
      "0.999999\n",
      "0.999289\n",
      "0.999939\n",
      "0.851838\n",
      "0.999747\n",
      "0.999726\n",
      "0.783156\n",
      "0.99974\n",
      "0.531218\n",
      "0.999997\n",
      "1.0\n",
      "0.649304\n",
      "0.979967\n",
      "1.0\n",
      "0.999842\n",
      "0.999866\n",
      "0.813271\n",
      "0.979937\n",
      "0.999563\n",
      "0.965282\n",
      "0.999996\n",
      "0.999749\n",
      "0.998956\n",
      "0.998606\n",
      "0.995402\n",
      "0.539565\n",
      "0.827101\n",
      "0.80216\n",
      "0.986732\n",
      "0.982731\n",
      "0.567848\n",
      "1.0\n",
      "0.998985\n",
      "0.997859\n",
      "0.999998\n",
      "0.999892\n",
      "0.999983\n",
      "0.999996\n",
      "1.0\n",
      "0.999782\n",
      "0.976814\n",
      "0.876223\n",
      "1.0\n",
      "0.959248\n",
      "0.991201\n",
      "0.98174\n",
      "0.800831\n",
      "0.753294\n",
      "0.999997\n",
      "0.999997\n",
      "0.999999\n",
      "0.999145\n",
      "0.865437\n",
      "0.956713\n",
      "0.999998\n",
      "0.905598\n",
      "0.999999\n",
      "0.99994\n",
      "0.999358\n",
      "0.999733\n",
      "0.59172\n",
      "0.999992\n",
      "1.0\n",
      "0.999995\n",
      "0.985974\n",
      "0.99985\n",
      "0.998875\n",
      "0.999887\n",
      "0.999971\n",
      "0.999342\n",
      "1.0\n",
      "0.999258\n",
      "0.995371\n",
      "0.995689\n",
      "0.959012\n",
      "0.617436\n",
      "0.696775\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.822135\n",
      "0.999966\n",
      "1.0\n",
      "0.925704\n",
      "0.573146\n",
      "0.999995\n",
      "0.999738\n",
      "0.999942\n",
      "0.999994\n",
      "0.998863\n",
      "0.965183\n",
      "1.0\n",
      "0.717865\n",
      "0.973127\n",
      "1.0\n",
      "0.998842\n",
      "0.998257\n",
      "0.999989\n",
      "1.0\n",
      "0.999928\n",
      "0.999766\n",
      "1.0\n",
      "0.999999\n",
      "1.0\n",
      "0.994365\n",
      "0.926373\n",
      "0.821767\n",
      "0.999954\n",
      "0.892873\n",
      "0.980365\n",
      "0.729688\n",
      "0.955091\n",
      "0.839991\n",
      "0.774546\n",
      "0.633111\n",
      "0.999958\n",
      "0.999164\n",
      "0.990607\n",
      "0.973001\n",
      "0.756419\n",
      "1.0\n",
      "0.89094\n",
      "0.999943\n",
      "0.973951\n",
      "0.988517\n",
      "0.909051\n",
      "0.983203\n",
      "0.991269\n",
      "1.0\n",
      "0.50777\n",
      "0.994526\n",
      "0.976395\n",
      "0.984421\n",
      "0.99468\n",
      "0.999997\n",
      "0.996232\n",
      "0.999996\n",
      "0.997648\n",
      "0.949061\n",
      "0.999985\n",
      "1.0\n",
      "0.983701\n",
      "0.999992\n",
      "0.656406\n",
      "0.999999\n",
      "0.993267\n",
      "0.996377\n",
      "0.999986\n",
      "0.998494\n",
      "0.965935\n",
      "0.992299\n",
      "0.999592\n",
      "0.979788\n",
      "0.872432\n",
      "0.9991\n",
      "0.995601\n",
      "0.999997\n",
      "0.865957\n",
      "0.988611\n",
      "0.918602\n",
      "0.692137\n",
      "0.999745\n",
      "0.985658\n",
      "0.983439\n",
      "0.995241\n",
      "0.940485\n",
      "0.996218\n",
      "1.0\n",
      "0.999999\n",
      "0.997012\n",
      "0.81429\n",
      "1.0\n",
      "0.978042\n",
      "0.97876\n",
      "0.562036\n",
      "0.997544\n",
      "0.992891\n",
      "0.99486\n",
      "0.999036\n",
      "0.86672\n",
      "0.99733\n",
      "0.899944\n",
      "0.859268\n",
      "0.942618\n",
      "0.998935\n",
      "1.0\n",
      "0.999273\n",
      "0.999971\n",
      "1.0\n",
      "0.92359\n",
      "0.910334\n",
      "0.889445\n",
      "0.995744\n",
      "0.954647\n",
      "1.0\n",
      "0.998941\n",
      "0.999846\n",
      "0.999953\n",
      "0.943838\n",
      "0.973501\n",
      "0.998824\n",
      "0.974149\n",
      "0.962729\n",
      "1.0\n",
      "0.833212\n",
      "0.995958\n",
      "0.999923\n",
      "0.999999\n",
      "0.999849\n",
      "0.961891\n",
      "0.962176\n",
      "0.504245\n",
      "0.999922\n",
      "0.989277\n",
      "0.999998\n",
      "1.0\n",
      "0.99976\n",
      "0.999648\n",
      "0.744691\n",
      "0.999939\n",
      "0.512162\n",
      "0.999311\n",
      "0.945463\n",
      "0.850543\n",
      "0.996044\n",
      "0.66527\n",
      "0.997014\n",
      "0.999955\n",
      "0.957374\n",
      "0.773197\n",
      "0.999966\n",
      "0.623409\n",
      "0.999999\n",
      "0.984648\n",
      "0.964719\n",
      "0.999957\n",
      "1.0\n",
      "0.825022\n",
      "0.986415\n",
      "1.0\n",
      "0.999999\n",
      "0.866672\n",
      "0.982229\n",
      "0.999595\n",
      "0.962384\n",
      "0.999995\n",
      "0.999988\n",
      "0.998935\n",
      "0.7891\n",
      "0.917839\n",
      "0.999426\n",
      "0.989834\n",
      "0.619817\n",
      "0.991865\n",
      "0.999914\n",
      "1.0\n",
      "0.842649\n",
      "0.972727\n",
      "0.995233\n",
      "1.0\n",
      "0.764682\n",
      "0.706744\n",
      "0.669466\n",
      "0.518113\n",
      "0.99983\n",
      "0.887122\n",
      "0.999915\n",
      "0.93688\n",
      "0.999983\n",
      "0.806644\n",
      "0.99648\n",
      "0.993148\n",
      "0.998212\n",
      "0.698555\n",
      "0.990766\n",
      "0.999914\n",
      "0.996569\n",
      "0.99046\n",
      "0.605592\n",
      "0.999998\n",
      "0.924205\n",
      "0.999983\n",
      "0.998589\n",
      "0.999837\n",
      "0.961965\n",
      "0.650764\n",
      "1.0\n",
      "0.758532\n",
      "1.0\n",
      "0.994496\n",
      "0.999841\n",
      "0.99637\n",
      "0.999992\n",
      "1.0\n",
      "1.0\n",
      "0.999999\n",
      "0.999907\n",
      "0.999994\n",
      "0.578861\n",
      "0.999996\n",
      "0.811265\n",
      "0.999836\n",
      "0.999548\n",
      "1.0\n",
      "0.683986\n",
      "0.999972\n",
      "0.999993\n",
      "0.980599\n",
      "0.995271\n",
      "0.999945\n",
      "0.995692\n",
      "0.874401\n",
      "0.999992\n",
      "0.978995\n",
      "0.999834\n",
      "0.99986\n",
      "0.995872\n",
      "0.998078\n",
      "0.98654\n",
      "0.998342\n",
      "0.999992\n",
      "0.999449\n",
      "1.0\n",
      "0.579635\n",
      "0.999999\n",
      "0.623676\n",
      "0.825426\n",
      "0.907555\n",
      "0.769274\n",
      "0.796337\n",
      "0.909379\n",
      "0.945188\n",
      "0.999981\n",
      "1.0\n",
      "0.99084\n",
      "0.999945\n",
      "0.943375\n",
      "0.986331\n",
      "0.999965\n",
      "0.93462\n",
      "1.0\n",
      "0.999951\n",
      "0.997491\n",
      "0.996747\n",
      "0.99956\n",
      "0.932764\n",
      "0.999853\n",
      "0.999881\n",
      "0.99809\n",
      "0.999805\n",
      "1.0\n",
      "0.999994\n",
      "0.978884\n",
      "0.99978\n",
      "0.94455\n",
      "0.997789\n",
      "0.531316\n",
      "0.975462\n",
      "0.994608\n",
      "0.999518\n",
      "0.719296\n",
      "0.697206\n",
      "0.998138\n",
      "0.999557\n",
      "0.99978\n",
      "0.9949\n",
      "0.999978\n",
      "0.999261\n",
      "0.926064\n",
      "0.999983\n",
      "0.99983\n",
      "0.86006\n",
      "0.992972\n",
      "0.999991\n",
      "0.999588\n",
      "0.94995\n",
      "0.999319\n",
      "0.882588\n",
      "0.73591\n",
      "0.999998\n",
      "0.999921\n",
      "0.764976\n",
      "0.999967\n",
      "0.999981\n",
      "0.999905\n",
      "0.961724\n",
      "0.998485\n",
      "0.999981\n",
      "0.772984\n",
      "0.551228\n",
      "0.914603\n",
      "0.981331\n",
      "1.0\n",
      "0.734087\n",
      "0.849476\n",
      "0.569485\n",
      "0.831113\n",
      "0.999997\n",
      "0.999999\n",
      "0.892948\n",
      "0.689458\n",
      "0.994449\n",
      "0.999879\n",
      "0.999995\n",
      "1.0\n",
      "0.999585\n",
      "0.999238\n",
      "0.843458\n",
      "0.999763\n",
      "0.526048\n",
      "0.939478\n",
      "0.76648\n",
      "0.924372\n",
      "0.784701\n",
      "0.996206\n",
      "0.737765\n",
      "0.951527\n",
      "0.845601\n",
      "0.95499\n",
      "0.994747\n",
      "1.0\n",
      "0.999992\n",
      "0.999261\n",
      "0.615136\n",
      "1.0\n",
      "0.972666\n",
      "0.929835\n",
      "0.99998\n",
      "0.972247\n",
      "0.547831\n",
      "1.0\n",
      "0.998646\n",
      "0.996122\n",
      "0.999976\n",
      "0.999638\n",
      "0.999816\n",
      "0.998857\n",
      "0.988177\n",
      "0.999619\n",
      "0.997138\n",
      "0.997441\n",
      "0.998681\n",
      "0.999912\n",
      "0.974348\n",
      "0.998716\n",
      "0.983656\n",
      "0.999977\n",
      "0.766399\n",
      "0.993788\n",
      "0.996066\n",
      "0.999739\n",
      "0.999917\n",
      "0.999967\n",
      "0.899335\n",
      "0.988568\n",
      "0.996584\n",
      "0.986655\n",
      "0.6287\n",
      "0.999923\n",
      "0.997904\n",
      "1.0\n",
      "1.0\n",
      "0.893323\n",
      "0.998262\n",
      "0.882891\n",
      "0.999973\n",
      "0.999998\n",
      "0.999964\n",
      "0.999998\n",
      "1.0\n",
      "0.999722\n",
      "0.863175\n",
      "1.0\n",
      "0.568863\n",
      "0.999951\n",
      "0.969692\n",
      "0.999796\n",
      "0.999998\n",
      "0.991334\n",
      "0.999357\n",
      "0.999052\n",
      "0.99572\n",
      "0.999997\n",
      "0.93135\n",
      "0.975811\n",
      "0.925322\n",
      "0.999247\n",
      "0.999939\n",
      "0.982026\n",
      "0.999128\n",
      "0.998867\n",
      "0.999367\n",
      "0.782162\n",
      "0.99991\n",
      "0.995622\n",
      "0.832239\n",
      "0.999984\n",
      "0.514278\n",
      "0.994457\n",
      "0.422921\n",
      "0.999989\n",
      "0.999617\n",
      "0.999988\n",
      "0.639622\n",
      "0.874207\n",
      "0.977102\n",
      "0.999879\n",
      "0.999988\n",
      "0.999406\n",
      "1.0\n",
      "0.892161\n",
      "1.0\n",
      "0.698469\n",
      "0.956585\n",
      "0.999805\n",
      "0.828231\n",
      "0.537077\n",
      "0.974036\n",
      "0.999977\n",
      "0.731574\n",
      "0.9998\n",
      "0.999999\n",
      "0.502164\n",
      "0.994407\n",
      "0.999963\n",
      "0.800289\n",
      "0.999998\n",
      "0.999195\n",
      "0.844391\n",
      "0.999998\n",
      "0.998723\n",
      "0.983034\n",
      "0.999942\n",
      "0.515897\n",
      "0.985595\n",
      "0.648258\n",
      "0.572872\n",
      "0.999937\n",
      "1.0\n",
      "0.964192\n",
      "0.754541\n",
      "0.995054\n",
      "0.971816\n",
      "0.553568\n",
      "0.993727\n",
      "0.563772\n",
      "0.999955\n",
      "0.98235\n",
      "0.999942\n",
      "0.979889\n",
      "0.998976\n",
      "0.954269\n",
      "0.98727\n",
      "0.999522\n",
      "0.986963\n",
      "0.999857\n",
      "0.702423\n",
      "1.0\n",
      "0.539685\n",
      "0.999838\n",
      "0.999909\n",
      "0.918816\n",
      "0.997342\n",
      "0.83518\n",
      "0.940897\n",
      "0.999928\n",
      "0.83138\n",
      "0.545987\n",
      "0.949152\n",
      "0.702699\n",
      "1.0\n",
      "0.999895\n",
      "0.975039\n",
      "0.994319\n",
      "0.921315\n",
      "0.999975\n",
      "0.957121\n",
      "0.999939\n",
      "0.95721\n",
      "0.8671\n",
      "0.801854\n",
      "1.0\n",
      "0.689148\n",
      "0.915808\n",
      "0.902311\n",
      "0.931946\n",
      "0.999968\n",
      "0.998249\n",
      "0.996957\n",
      "0.984317\n",
      "0.995062\n",
      "0.986738\n",
      "0.98017\n",
      "0.622818\n",
      "0.970166\n",
      "1.0\n",
      "0.971152\n",
      "0.999999\n",
      "0.587404\n",
      "0.60911\n",
      "0.99261\n",
      "1.0\n",
      "0.661428\n",
      "0.970955\n",
      "0.986982\n",
      "0.993595\n",
      "0.926811\n",
      "0.999859\n",
      "1.0\n",
      "0.999998\n",
      "0.999998\n",
      "0.993448\n",
      "0.855663\n",
      "0.502483\n",
      "0.999886\n",
      "0.999881\n",
      "0.951098\n",
      "0.909859\n",
      "0.546292\n",
      "0.996217\n",
      "0.89529\n",
      "0.999992\n",
      "0.677901\n",
      "0.568707\n",
      "0.999998\n",
      "0.873814\n",
      "1.0\n",
      "0.999944\n",
      "0.80561\n",
      "0.999995\n",
      "0.77694\n",
      "1.0\n",
      "0.998165\n",
      "0.999479\n",
      "0.999513\n",
      "0.847884\n",
      "0.566101\n",
      "0.624854\n",
      "0.999838\n",
      "0.984793\n",
      "0.996178\n",
      "0.974385\n",
      "0.999828\n",
      "0.906145\n",
      "0.983171\n",
      "0.965859\n",
      "0.886451\n",
      "0.991605\n",
      "0.993363\n",
      "0.999929\n",
      "0.99303\n",
      "0.987628\n",
      "0.996411\n",
      "0.978648\n",
      "0.999998\n",
      "0.979685\n",
      "1.0\n",
      "0.534915\n",
      "0.99916\n",
      "0.998834\n",
      "0.999837\n",
      "0.998818\n",
      "0.792597\n",
      "0.999215\n",
      "0.999023\n",
      "0.929916\n",
      "0.979672\n",
      "1.0\n",
      "0.988637\n",
      "0.943272\n",
      "0.99997\n",
      "0.540788\n",
      "0.482272\n",
      "0.931654\n",
      "1.0\n",
      "0.996208\n",
      "0.999999\n",
      "0.966736\n",
      "0.999753\n",
      "0.728182\n",
      "0.998849\n",
      "0.560989\n",
      "0.999934\n",
      "0.999087\n",
      "1.0\n",
      "0.52543\n",
      "0.999846\n",
      "0.730722\n",
      "0.848024\n",
      "0.979891\n",
      "0.999992\n",
      "0.999092\n",
      "0.999938\n",
      "0.999925\n",
      "0.998625\n",
      "0.999879\n",
      "0.55893\n",
      "0.999995\n",
      "0.999998\n",
      "0.987594\n",
      "0.497248\n",
      "0.91228\n",
      "0.567096\n",
      "0.999789\n",
      "1.0\n",
      "0.808873\n",
      "0.998333\n",
      "0.999999\n",
      "0.795092\n",
      "0.999274\n",
      "0.90381\n",
      "0.984939\n",
      "0.819239\n",
      "0.521292\n",
      "0.998897\n",
      "0.853449\n",
      "0.933982\n",
      "1.0\n",
      "0.970505\n",
      "0.999063\n",
      "0.997539\n",
      "0.998046\n",
      "0.999995\n",
      "0.999454\n",
      "0.978412\n",
      "1.0\n",
      "0.999823\n",
      "0.58448\n",
      "0.945062\n",
      "0.999995\n",
      "0.999972\n",
      "0.976429\n",
      "1.0\n",
      "0.975222\n",
      "0.71072\n",
      "0.999988\n",
      "0.46847\n",
      "0.959238\n",
      "0.986358\n",
      "0.94823\n",
      "0.999706\n",
      "0.940329\n",
      "0.715919\n",
      "0.978745\n",
      "0.998086\n",
      "0.999955\n",
      "0.993849\n",
      "0.797387\n",
      "0.999999\n",
      "0.999892\n",
      "0.996636\n",
      "0.997276\n",
      "0.958964\n",
      "0.645006\n",
      "1.0\n",
      "0.997431\n",
      "0.998503\n",
      "0.997204\n",
      "0.99996\n",
      "0.910629\n",
      "0.999976\n",
      "0.615895\n",
      "0.999915\n",
      "0.995389\n",
      "0.999281\n",
      "0.982876\n",
      "0.99362\n",
      "0.994276\n",
      "0.999975\n",
      "0.985249\n",
      "0.99384\n",
      "0.973287\n",
      "0.799702\n",
      "0.999996\n",
      "0.999699\n",
      "0.999931\n",
      "0.989379\n",
      "1.0\n",
      "0.912914\n",
      "0.997308\n",
      "0.999875\n",
      "0.999922\n",
      "0.999981\n",
      "0.955453\n",
      "1.0\n",
      "1.0\n",
      "0.999966\n",
      "0.987195\n",
      "0.889873\n",
      "0.992292\n",
      "0.955842\n",
      "1.0\n",
      "0.907617\n",
      "0.999562\n",
      "0.99998\n",
      "1.0\n",
      "0.960118\n",
      "0.999923\n",
      "0.683534\n",
      "0.932137\n",
      "0.995092\n",
      "0.999099\n",
      "0.664847\n",
      "0.990251\n",
      "0.91077\n",
      "0.999998\n",
      "0.999996\n",
      "1.0\n",
      "1.0\n",
      "0.999999\n",
      "0.801437\n",
      "0.999399\n",
      "0.570565\n",
      "0.999997\n",
      "0.986253\n",
      "0.812543\n",
      "0.555681\n",
      "0.999977\n",
      "0.837057\n",
      "0.999999\n",
      "0.999181\n",
      "0.999249\n",
      "0.942127\n",
      "0.998576\n",
      "0.999868\n",
      "0.673254\n",
      "0.668366\n",
      "0.999975\n",
      "0.704962\n",
      "0.999997\n",
      "0.749049\n",
      "1.0\n",
      "0.999978\n",
      "0.999771\n",
      "0.999925\n",
      "0.999958\n",
      "0.999994\n",
      "0.997267\n",
      "0.999335\n",
      "1.0\n",
      "0.999996\n",
      "0.999423\n",
      "0.859406\n",
      "0.999494\n",
      "0.635123\n",
      "0.966661\n",
      "0.790799\n",
      "0.999997\n",
      "0.998209\n",
      "0.998858\n",
      "0.999999\n",
      "1.0\n",
      "1.0\n",
      "0.803996\n",
      "0.999779\n",
      "0.986544\n",
      "0.933575\n",
      "0.999978\n",
      "1.0\n",
      "0.863707\n",
      "0.731674\n",
      "0.747473\n",
      "1.0\n",
      "0.598108\n",
      "0.836504\n",
      "0.997769\n",
      "0.999651\n",
      "0.550225\n",
      "0.999935\n",
      "0.909083\n",
      "0.710013\n",
      "0.999235\n",
      "0.999544\n",
      "0.985548\n",
      "0.873441\n",
      "0.97483\n",
      "0.875225\n",
      "0.999964\n",
      "0.63533\n",
      "0.963562\n",
      "0.999629\n",
      "0.998608\n",
      "0.999556\n",
      "1.0\n",
      "0.665438\n",
      "1.0\n",
      "0.99854\n",
      "0.925909\n",
      "0.704529\n",
      "0.588857\n",
      "1.0\n",
      "0.778647\n",
      "0.999869\n",
      "0.999997\n",
      "0.998722\n",
      "0.995044\n",
      "0.999962\n",
      "0.974344\n",
      "0.999998\n",
      "0.999997\n",
      "0.999977\n",
      "0.999926\n",
      "0.914849\n",
      "0.999622\n",
      "0.996859\n",
      "0.996759\n",
      "0.997837\n",
      "0.99981\n",
      "0.702218\n",
      "0.999137\n",
      "0.641976\n",
      "0.840784\n",
      "0.973817\n",
      "0.984852\n",
      "0.999918\n",
      "0.999923\n",
      "0.51589\n",
      "0.727302\n",
      "0.955658\n",
      "0.754919\n",
      "1.0\n",
      "0.999995\n",
      "0.977649\n",
      "0.998404\n",
      "0.999998\n",
      "0.999066\n",
      "0.868427\n",
      "0.996986\n",
      "0.955793\n",
      "0.999997\n",
      "0.999431\n",
      "0.711686\n",
      "0.999369\n",
      "0.788248\n",
      "0.989182\n",
      "0.893525\n",
      "0.785561\n",
      "0.983676\n",
      "1.0\n",
      "0.999476\n",
      "1.0\n",
      "0.902839\n",
      "0.933729\n",
      "0.946676\n",
      "1.0\n",
      "0.999987\n",
      "0.97387\n",
      "0.99713\n",
      "1.0\n",
      "0.998399\n",
      "0.998819\n",
      "1.0\n",
      "0.999579\n",
      "0.986109\n",
      "0.981493\n",
      "0.592482\n",
      "1.0\n",
      "0.964212\n",
      "0.992376\n",
      "0.99592\n",
      "0.876327\n",
      "0.510121\n",
      "0.607292\n",
      "0.838854\n",
      "0.880534\n",
      "0.999517\n",
      "0.529252\n",
      "0.952395\n",
      "0.999911\n",
      "0.996594\n",
      "0.995169\n",
      "0.905283\n",
      "0.952379\n",
      "0.999997\n",
      "0.946623\n",
      "0.999998\n",
      "0.991706\n",
      "0.999996\n",
      "0.994611\n",
      "0.999385\n",
      "0.9993\n",
      "0.993292\n",
      "0.998065\n",
      "0.999979\n",
      "0.999151\n",
      "0.671463\n",
      "0.999193\n",
      "0.999007\n",
      "1.0\n",
      "0.849867\n",
      "0.999722\n",
      "0.754588\n",
      "0.842879\n",
      "0.618205\n",
      "0.688364\n",
      "0.722637\n",
      "0.910897\n",
      "0.978005\n",
      "0.954536\n",
      "0.99815\n",
      "0.744353\n",
      "0.999959\n",
      "0.984019\n",
      "0.855245\n",
      "0.938286\n",
      "0.940181\n",
      "0.999992\n",
      "0.933392\n",
      "0.999699\n",
      "0.999976\n",
      "0.59844\n",
      "0.963506\n",
      "0.999997\n",
      "0.999992\n",
      "0.990382\n",
      "0.999605\n",
      "0.928604\n",
      "0.978906\n",
      "0.994104\n",
      "0.999428\n",
      "0.971229\n",
      "0.999837\n",
      "0.915678\n",
      "0.844038\n",
      "0.927016\n",
      "0.614966\n",
      "0.999994\n",
      "0.999189\n",
      "0.551615\n",
      "1.0\n",
      "0.94786\n",
      "0.999985\n",
      "0.954586\n",
      "0.999982\n",
      "1.0\n",
      "0.955294\n",
      "0.923657\n",
      "0.999999\n",
      "0.999447\n",
      "0.848643\n",
      "1.0\n",
      "0.999917\n",
      "0.999999\n",
      "1.0\n",
      "0.947007\n",
      "0.949119\n",
      "0.999558\n",
      "0.819981\n",
      "0.659934\n",
      "0.517731\n",
      "0.999998\n",
      "0.999995\n",
      "0.999971\n",
      "0.93029\n",
      "0.999995\n",
      "0.983434\n",
      "0.887956\n",
      "0.976657\n",
      "0.959744\n",
      "0.995829\n",
      "0.999342\n",
      "0.965718\n",
      "0.846159\n",
      "0.999993\n",
      "0.999995\n",
      "0.999223\n",
      "0.998805\n",
      "0.999993\n",
      "0.603708\n",
      "0.713536\n",
      "0.983627\n",
      "1.0\n",
      "0.99998\n",
      "0.996856\n",
      "0.987201\n",
      "0.999998\n",
      "0.999113\n",
      "0.75617\n",
      "0.836464\n",
      "0.604258\n",
      "0.99999\n",
      "0.973677\n",
      "0.818874\n",
      "0.991368\n",
      "0.99961\n",
      "0.999518\n",
      "0.565487\n",
      "0.999587\n",
      "0.999604\n",
      "0.998063\n",
      "0.965862\n",
      "0.844059\n",
      "0.999998\n",
      "0.998652\n",
      "0.997209\n",
      "0.999322\n",
      "0.99125\n",
      "0.805344\n",
      "0.999954\n",
      "0.974293\n",
      "0.937681\n",
      "1.0\n",
      "0.999846\n",
      "0.928294\n",
      "0.915058\n",
      "0.913842\n",
      "0.501503\n",
      "0.998599\n",
      "1.0\n",
      "0.995118\n",
      "1.0\n",
      "0.677143\n",
      "0.885968\n",
      "0.92183\n",
      "0.873905\n",
      "0.999545\n",
      "0.999575\n",
      "0.999178\n",
      "0.989323\n",
      "0.95516\n",
      "0.909939\n",
      "0.938071\n",
      "0.853517\n",
      "0.990478\n",
      "0.965281\n",
      "0.967878\n",
      "0.999663\n",
      "0.999992\n",
      "0.999482\n",
      "0.538898\n",
      "0.999394\n",
      "0.915002\n",
      "0.999949\n",
      "0.914458\n",
      "0.999856\n",
      "1.0\n",
      "0.998187\n",
      "0.99861\n",
      "0.999961\n",
      "0.999998\n",
      "0.997221\n",
      "0.999795\n",
      "0.973455\n",
      "0.769862\n",
      "0.969342\n",
      "0.999999\n",
      "0.999996\n",
      "0.982559\n",
      "0.976157\n"
     ]
    }
   ],
   "source": [
    "adv_predict=predict_fn(adv_x)\n",
    "adv_conf=confidence(adv_x)\n",
    "for i in range(5000):\n",
    "    if adv_predict[i]!=y_curr[i]:\n",
    "        print adv_conf[i,adv_predict[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5266000032424927"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
